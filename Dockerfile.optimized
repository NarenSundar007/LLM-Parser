# Multi-stage build for optimized LLM deployment
# Stage 1: Base dependencies and model download
FROM python:3.11-slim as base

# Install system dependencies
RUN apt-get update && apt-get install -y \
    gcc \
    g++ \
    curl \
    && rm -rf /var/lib/apt/lists/*

# Set working directory
WORKDIR /app

# Copy requirements and install dependencies
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Pre-download ML models to cache them in the image
RUN python -c "\
from sentence_transformers import SentenceTransformer; \
import os; \
os.environ['TRANSFORMERS_CACHE'] = '/app/models'; \
model = SentenceTransformer('all-MiniLM-L6-v2'); \
print('âœ… SentenceTransformer model cached')"

# Stage 2: Production image
FROM python:3.11-slim as production

# Install minimal runtime dependencies
RUN apt-get update && apt-get install -y \
    curl \
    && rm -rf /var/lib/apt/lists/*

# Create non-root user for security
RUN useradd --create-home --shell /bin/bash app

# Set working directory
WORKDIR /app

# Copy Python packages from base stage
COPY --from=base /usr/local/lib/python3.11/site-packages /usr/local/lib/python3.11/site-packages
COPY --from=base /usr/local/bin /usr/local/bin

# Copy cached models
COPY --from=base /app/models /app/models
ENV TRANSFORMERS_CACHE=/app/models

# Copy application code
COPY --chown=app:app . .

# Create data directories
RUN mkdir -p data/uploads data/faiss_index && \
    chown -R app:app data/

# Switch to non-root user
USER app

# Expose port (use env var for flexibility)
EXPOSE ${PORT:-8000}

# Health check using the new readiness endpoint
HEALTHCHECK --interval=30s --timeout=10s --start-period=60s --retries=3 \
    CMD curl -f http://localhost:${PORT:-8000}/ready || exit 1

# Production startup command
CMD ["sh", "-c", "uvicorn main:app --host 0.0.0.0 --port ${PORT:-8000} --workers 1"]
