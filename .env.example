## Environment Variables for LLM-Powered Query-Retrieval System
# Copy this file to .env and replace placeholder values with your actual API keys

# Primary: Gemini API Key (get from https://aistudio.google.com/app/apikey)
GEMINI_API_KEY=AIzaSyCg-dkchX3L0yYIxXdxvOsG5ZiZBbTCtKM

# Alternative: Groq API Key (get from https://console.groq.com)
GROQ_API_KEY=gsk_your_groq_api_key_here

# Optional: OpenAI API Key (for alternative LLM provider)
OPENAI_API_KEY=sk-your_openai_api_key_here

# Optional: Pinecone API Key (for cloud vector database)
PINECONE_API_KEY=your_pinecone_api_key_here
PINECONE_ENVIRONMENT=us-east-1-aws

# LLM Configuration
LLM_PROVIDER=gemini
LLM_MODEL=gemini-1.5-flash
EMBEDDING_MODEL=sentence-transformers
MAX_TOKENS=2000
TEMPERATURE=0.1

# Vector Database Configuration
USE_PINECONE=false
PINECONE_INDEX_NAME=document-retrieval
FAISS_INDEX_PATH=./data/faiss_index

# Document Processing
CHUNK_SIZE=200
CHUNK_OVERLAP=40
MAX_CHUNKS_PER_DOC=1000

# API Configuration
API_HOST=0.0.0.0
API_PORT=8000
DEBUG=true
LOG_LEVEL=INFO

# Data Directories
DATA_DIR=./data
UPLOADS_DIR=./data/uploadsnment Variables - COPY TO .env AND ADD YOUR REAL KEYS
GROQ_API_KEY=your_groq_api_key_here
OPENAI_API_KEY=your_openai_api_key_here
PINECONE_API_KEY=your_pinecone_api_key_herevironment Variables
GROQ_API_KEY="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"
OPENAI_API_KEY="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"  # Optional: for embeddings only if no HuggingFace
PINECONE_API_KEY="pcsk_6L3wxq_6DCuLqiKWpHXQD7MtY3wySrNnFrzmKWX1BmXAJmfQQQ3XuaFgguzUxmmTHM779N"
PINECONE_ENVIRONMENT=us-east-1-aws  # Replace with your actual environment (e.g., us-east-1-aws, gcp-starter, etc.)

# LLM Configuration (Groq) - OPTIMIZED FOR FREE USAGE
LLM_PROVIDER=groq  # Using Groq for FREE LLM access
LLM_MODEL=llama-3.1-70b-versatile  # Best free Groq model for accuracy
EMBEDDING_MODEL=sentence-transformers  # FREE local embeddings (no API costs)
MAX_TOKENS=2000
TEMPERATURE=0.1

# Vector Database
USE_PINECONE=false  # Set to true to use Pinecone, false for FAISS
PINECONE_INDEX_NAME=document-retrieval
FAISS_INDEX_PATH=./data/faiss_index

# Document Processing
CHUNK_SIZE=200
CHUNK_OVERLAP=40
MAX_CHUNKS_PER_DOC=1000

# API Configuration
API_HOST=0.0.0.0
API_PORT=8000
DEBUG=true

# Logging
LOG_LEVEL=INFO
